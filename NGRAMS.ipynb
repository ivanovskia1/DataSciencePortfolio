{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    words = word_tokenize(review.text)\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    reviews1.append(lemma_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(hotel_1)\n",
    "print(len(hotel_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and test subsets\n",
    "train_set1, test_set1 = hotel_1[:181], hotel_1[181:]\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set1)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set1):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "#precision - Out of the ones our model said will be (pos/neg), how many did we predict correctly ? \n",
    "#recall - Out of the ones that were (pos/neg), how many did we predict correctly? \n",
    "print(\"Naive Bayes Performance Based on Hotel_1 \")    \n",
    "print(\"Accuracy on same hotel:\",nltk.classify.accuracy(classifier, test_set1))\n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training and test subsets\n",
    "test_set2 = hotel_2\n",
    "for i, (feats, label) in enumerate(test_set2):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "#precision - Out of the ones our model said will be (pos/neg), how many did we predict correctly ? \n",
    "#recall - Out of the ones that were (pos/neg), how many did we predict correctly? \n",
    "print(\"Naive Bayes Performance Based on Hotel_2 \")    \n",
    "print(\"Accuracy on new hotel:\",nltk.classify.accuracy(classifier, test_set2))\n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))\n",
    "\n",
    "\n",
    "\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections \n",
    "from nltk import metrics\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set1, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "\n",
    "\n",
    "\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "\n",
    "#classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set1):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "#precision - Out of the ones our model said will be (pos/neg), how many did we predict correctly ? \n",
    "#recall - Out of the ones that were (pos/neg), how many did we predict correctly? \n",
    "print(\"Decision Tree Based on Hotel_1\")    \n",
    "print(\"Accuracy on hotel:\",nltk.classify.accuracy(dt_classifier, test_set1))\n",
    "print('pos precision:', precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))\n",
    "\n",
    "# Display the rules in pseudocode\n",
    "# Display the rules\n",
    "\n",
    "dt_classifier.pretty_format(width=10, prefix='', depth=10)\n",
    "dt_classifier.pseudocode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "# Define a function that will tokenize and stem text\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    stems = [stemmer.stem(t) for t in clean_words]\n",
    "    return stems\n",
    "\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "for s in cuisines:\n",
    "    allwords_stemmed = tokenize_and_stem(s)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "  \n",
    "print(len(totalvocab_stemmed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=300000,\n",
    "                                 min_df=0.15, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(cuisines)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Total vocabulary: \", len(totalvocab_stemmed), \"Terms used in the TF-IDF matrix: \", len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)  \n",
    "    bigrams = bigram_finder.nbest(score_fn, n)  \n",
    "    return bag_of_words(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = zip(reviews,ratings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data2 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_bigrams_words(z)\n",
    "    train_data2.append((bag_of_bigrams_words(z),e))\n",
    "    \n",
    "print(train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_data2)\n",
    "print(len(train_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "nbrefset = collections.defaultdict(set)\n",
    "nbtestset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    nbrefset[label].add(i)\n",
    "    observed = nb_classifier.classify(feats)\n",
    "    nbtestset[observed].add(i)\n",
    "print(\"BigramNB Recall\")\n",
    "print('neg recall:', recall(nbtestset['neg'], nbrefset['neg']))\n",
    "nb_classifier.show_most_informative_features(n=5)\n",
    "\n",
    "\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"BigramDT Recall\")\n",
    "print('neg recall:', recall(testset['neg'], refset['neg']))\n",
    "\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"BigramsLogit Recall\")\n",
    "print('neg recall:', recall(testset['neg'], refset['neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=200):\n",
    "    Trigram_finder = TrigramCollocationFinder.from_words(words)  \n",
    "    Trigrams = Trigram_finder.nbest(score_fn, n)  \n",
    "    return trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = zip(reviews,ratings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data4 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_Ngrams_words(z)\n",
    "    train_data4.append((bag_of_Ngrams_words(z),e))\n",
    "    \n",
    "random.shuffle(train_data4)\n",
    "print(len(train_data4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    \n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)  \n",
    "    \n",
    "    bigrams = bigram_finder.nbest(score_fn, n)  \n",
    "    \n",
    "    return bigrams\n",
    "\n",
    "def trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=200):\n",
    "    Trigram_finder = TrigramCollocationFinder.from_words(words)  \n",
    "    Trigrams = Trigram_finder.nbest(score_fn, n)  \n",
    "    return trigrams\n",
    "\n",
    "def bag_of_Ngrams_words(words):\n",
    "    \n",
    "    bigramBag = bigrams_words(words)\n",
    "    \n",
    "    for b in range(len(reviews)):\n",
    "        bigramBag[b]=' '.join(bigramBag[b])\n",
    "        \n",
    "    trigramBag = trigrams_words(words)\n",
    "    \n",
    "    for t in range(len(reviews)):\n",
    "        trigramBag[t]=' '.join(trigramBag[t])\n",
    "        \n",
    "    return bag_of_words(trigramBag + bigramBag + words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
